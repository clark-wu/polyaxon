---
title: "HPTuning - Polyaxonfile YAML Specification"
sub_link: "polyaxonfile-yaml-specification/hptuning"
meta_title: "Hyperparameters Tuning Section - Polyaxonfile YAML Specification Sections - Polyaxon References"
meta_description: "Hyperparameters Tuning Section - Polyaxonfile YAML Specification Sections."
visibility: public
status: published
tags:
    - specifications
    - polyaxon
    - yaml
sidebar: "polyaxon-yaml-specification"
---

## Overview

The hptuning defines `seed`, `concurrency`, `search algorithm`, `early_stopping`, `matrix`.

In general the hptuning defines some values that must be unique for
all experiments created based on the polyaxonfile.

## seed

A seed to use when generating random values during the hyperparameters search.

Example:

```yaml
seed: 3234
```

## concurrency

Defines how many experiments to run concurrently when the polyaxon file use a `matrix` section.
This option will be ignored if the polyaxon file only have one independent experiment.

Example:

```yaml
concurrency: 3
```

## matrix

The matrix section works the same way as travisCI matrix section,
and it basically creates multiple specifications.
The way it does that is depend on the methods used to define the hyperparameters and the search algorithm:

 * In the case of grid search, the matrix space is defined by the Cartesian Product of all your defined parameters.
 it is important that only discrete methods are used with grid search.

 * In the case of random search, hyperband, and bayesian optimization, the space search is defined based on sampling,
 from the provided distributions, if discrete values are provided sampling is done uniformly, unless `pvalues` is provided.

The matrix also defines variables same way the `declarations` does, the only difference is
that all the values generated by the matrix contribute to the definition of an experiment group.
Each experiment in this group is defined based on a combination of the values declared in the matrix.

The matrix is defined as `{key: value}` object where the key is the name of the parameter
you are defining and the value is one of these options:


### Discrete values

 * **values**: a list of values, e.g.

    * `[1, 2, 3, 4]`

 * **range**: [start, stop, step] same way you would define a range in python, e.g.

    * `[1, 10, 2]`
    * `{start: 1, stop: 10, step: 2}`
    * `'1:10:2'`

 * **linspace**: [start, stop, num] steps from start to stop spaced evenly on a `linear scale`, e.g.

    * `[1, 10, 5]`
    * `{start: 1, stop: 10, num: 20}`
    * `'1:2:20'`

 * **logspace**: [start, stop, num] steps from start to stop spaced evenly on a `log scale`, e.g.

    * `[1, 10, 5]`
    * `{start: 1, stop: 10, num: 20}`
    * `'1:2:20'`

 * **geomspace**: [start, stop, num] steps from start to stop, numbers spaced evenly on a log scale (a geometric progression).

    * `[1, 10, 5]`
    * `{start: 1, stop: 10, num: 20}`
    * `'1:2:20'`

### Distributions

 * **pvalues**: Draws a value_i from values with probability  prob_i, e.g.

    * `[(value1, prob1), (value2, prob12), (value3, prob3), ...]`

 * **uniform**: Draws samples from a uniform distribution over the half-open interval `[low, high)`, e.g.

    * `0:1`
    * `[0, 1]`
    * `{'low': 0, 'high': 1}`

 * **quniform**: Draws samples from a quantized uniform distribution over [low, high], `round(uniform(low, high) / q) * q`, e.g.

    * `0:1:0.1`
    * `[0, 1, 0.1]`
    * `{'low': 0, 'high': 1, 'q': 0.1}`

 * **loguniform**: Draws samples from a log uniform distribution over [low, high], e.g.

    * `0:1`
    * `[0, 1]`
    * `{'low': 0, 'high': 1}`

 * **qloguniform**: Draws samples from a quantized log uniform distribution over [low, high]

    * `0:1:0.1`
    * `[0, 1, 0.1]`
    * `{'low': 0, 'high': 1, 'q': 0.1}`

 * **normal**: Draws random samples from a normal (Gaussian) distribution defined by [loc, scale]

    * `0:1`
    * `[0, 1]`
    * `{'loc': 0, 'loc': 1}`

 * **qnormal**: Draws random samples from a quantized normal (Gaussian) distribution defined by [loc, scale]

    * `0:1:0.1`
    * `[0, 1, 0.1]`
    * `{'low': 0, 'high': 1, 'q': 0.1}`

 * **lognormal**: Draws random samples from a log normal (Gaussian) distribution defined by [loc, scale]

    * `0:1`
    * `[0, 1]`
    * `{'loc': 0, 'loc': 1}`

 * **qlognormal**: Draws random samples from a quantized log normal (Gaussian) distribution defined by [loc, scale]

    * `0:1:0.1`
    * `[0, 1, 0.1]`
    * `{'low': 0, 'high': 1, 'q': 0.1}`


Example:

```yaml
matrix:
  lr:
    logspace: 0.01:0.1:5

  loss:
    values: [MeanSquaredError, AbsoluteDifference]
```
Or
```yaml
matrix:
  lr:
    uniform: 0.01:0.8

  loss:
    pvalues: [(MeanSquaredError, 0,2), (AbsoluteDifference, 0.8)]
```

These values can be accessed in the following way:

```yaml
--lr={{ lr }} --loss={{ loss }}
```

You can, of course, only access one generated value at a time,
and the value is chosen directly by the algorithm doing the search defined in the `hptuning`.

For each experiment generated during the hyperparameters search, Polyaxon will also add these values
to your declarations, and will export them under the environment variable name `POLYAXON_DECLARATIONS`.

> tip "Polyaxon append the matrix value combination to your declarations and export them under the environment variable name `POLYAXON_DECLARATIONS`" 
Check how you can [get the cluster definition](/references/polyaxon-tracking-api/experiments/#tracking-experiments-running-inside-polyaxon) to use it with your models.


## search algorithm: grid_search

Hyperparameters search using grid search. This the default value when no algorithm is provided.
By default, the grid search will travers all possible combinations based on the cartesian product,
unless `n_experiments` is provided.

Example:

```yaml
grid_search:
  n_experiments: 10
```

## search algorithm: random_search

Hyperparameters search using random search.

Example:

```yaml
random_search:
  n_experiments: 10
```

## search algorithm: hyperband

Hyperparameters search using hyperband.

Example:

```yaml
hyperband:
  max_iter: 81
  eta: 3
  resource:
    name: num_steps
    type: int
  metric:
    name: loss
    optimization: minimize
  resume: False
```

## search algorithm: bo

Hyperparameters search using bayesian optimization.

Example:

```yaml
bo:
  n_iterations: 15
  n_initial_trials: 30
  metric:
    name: loss
    optimization: minimize
  utility_function:
    acquisition_function: ucb
    kappa: 1.2
    gaussian_process:
      kernel: matern
      length_scale: 1.0
      nu: 1.9
      n_restarts_optimizer: 0
```

## early_stopping

Defines a list of metrics and the values for these metrics to stop the search algorithm.

Example:

```yaml
early_stopping:
  - metric: loss
    value: 0.01
    optimization: minimize

  - metric: accuracy
    value: 0.97
    optimization: maximize
```
